{
 "cells": [
  {
   "cell_type": "raw",
   "id": "229dfb0c-417a-417f-ad60-76f859028790",
   "metadata": {},
   "source": [
    "Name - Dhanraj Pallepati\n",
    "Course Number - HDS 5230\n",
    "Assignment Number - Week 05 Dask Programming Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7440f37a-5909-4e71-968f-404221299c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3dd91a-7d83-4aa0-8d12-e2ddf5e0f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'aggregate': 'object',\n",
    "    'cases': 'float64',\n",
    "    'city': 'object',\n",
    "    'population': 'float64',\n",
    "    'deaths': 'float64',\n",
    "    'country': 'object',\n",
    "    'state': 'object',\n",
    "    'date': 'object'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeaec67a-4e50-4ccf-91fc-945014cca3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(\"timeseries.csv\",dtype=dtypes)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d3e1d1e-3a29-4715-82b4-df4f4ff2f5eb",
   "metadata": {},
   "source": [
    "The operation makes perfect sense to parallelize because the CSV file size is considerable. Fire can improve file loading speed because it splits the file into portions which workers independently read. Parallelization of CSV reading activities offers substantial benefits for systems with multiple cores or machines because this operation exists in the I/O bound category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b71e5b-ab29-4028-9546-d5b36be518ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for US states only and state-level data\n",
    "us_df = df[(df['country'] == 'United States') & (df['aggregate'] == 'state')]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d6c45b0-727e-463a-b5e8-d5083bacb538",
   "metadata": {},
   "source": [
    "We derive us_df by filtering df to those records where country equals 'United States' while aggregate equals 'state'.\n",
    "The applicable partition parallelization for this filtering step would most likely produce marginal performance gains. The procedure remains straightforward while the distribution costs potentially surpass the achievable advantages. The current utilization of Dask DataFrame ensures we can parallelize subsequent operations because we maintain a distributed data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26c3454b-2627-4c5b-9b1a-d4bc89aed901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2021-02-28'\n",
    "mask = (us_df['date'] >= start_date) & (us_df['date'] <= end_date)\n",
    "period_df = us_df[mask]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d842a67-949f-48b3-b5f2-1dabf717b996",
   "metadata": {},
   "source": [
    "Date filtering operates in a fashion comparable to the country/state filtering strategy. Parallelization is possible for this operation but the distribution costs might exceed its benefits. The data distribution remains beneficial to maintain because the current filtered dataset possesses a substantial size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf0c00f-d96a-4b90-8b9f-2fc17be06295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [deaths, population, per_capita_mortality, mortality_rank]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "state_metrics = period_df.groupby('state').agg({\n",
    "    'deaths': 'sum',\n",
    "    'population': 'mean'\n",
    "}).compute()\n",
    "\n",
    "state_metrics['per_capita_mortality'] = state_metrics['deaths'] / state_metrics['population']\n",
    "state_metrics['mortality_rank'] = state_metrics['per_capita_mortality'].rank(ascending=False)\n",
    "\n",
    "print(state_metrics.sort_values('mortality_rank'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2378c482-55f5-4316-8ee1-ad33d6b1143d",
   "metadata": {},
   "source": [
    "A parallelization approach brings value to the groupby-aggregate operation because it requires consolidating information from extensive state datasets. The computation involves splitting state aggregations among separate workers which creates an embarrassingly parallel opportunity for this problem. The compute() command enables quick combination of data from all participating workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "074e7715-7d15-4ef6-ae19-85d3f5e5f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanr\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+-----------+---------+----------+\n| Column    | Found   | Expected |\n+-----------+---------+----------+\n| county    | object  | float64  |\n| recovered | float64 | int64    |\n+-----------+---------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- county\n  ValueError(\"could not convert string to float: 'Grant County'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'county': 'object',\n       'recovered': 'float64'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m period_df \u001b[38;5;241m=\u001b[39m period_df\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m      2\u001b[0m period_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(period_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m period_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth_year\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m period_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask_expr\\_collection.py:477\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    476\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\base.py:664\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 664\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask_expr\\_expr.py:3765\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[1;34m(graph, name, *deps)\u001b[0m\n\u001b[0;32m   3763\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[0;32m   3764\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[1;32m-> 3765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dask\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mget(graph, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas_read_text(\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader,\n\u001b[0;32m    144\u001b[0m     block,\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[0;32m    146\u001b[0m     rest_kwargs,\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypes,\n\u001b[0;32m    148\u001b[0m     columns,\n\u001b[0;32m    149\u001b[0m     write_header,\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce,\n\u001b[0;32m    151\u001b[0m     path_info,\n\u001b[0;32m    152\u001b[0m )\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[1;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[1;32m--> 197\u001b[0m     coerce_dtypes(df, dtypes)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[1;34m(df, dtypes)\u001b[0m\n\u001b[0;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[0;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[0;32m    297\u001b[0m )\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+-----------+---------+----------+\n| Column    | Found   | Expected |\n+-----------+---------+----------+\n| county    | object  | float64  |\n| recovered | float64 | int64    |\n+-----------+---------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- county\n  ValueError(\"could not convert string to float: 'Grant County'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'county': 'object',\n       'recovered': 'float64'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "period_df = period_df.compute()\n",
    "period_df['date'] = pd.to_datetime(period_df['date'])\n",
    "period_df['month_year'] = period_df['date'].dt.strftime('%Y-%m')\n",
    "monthly_metrics = period_df.groupby(['state', 'month_year']).agg({\n",
    "    'deaths': 'last',\n",
    "    'cases': 'last'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate CFR (Case Fatality Rate)\n",
    "monthly_metrics['cfr'] = monthly_metrics['deaths'] / monthly_metrics['cases']\n",
    "cfr_matrix = monthly_metrics.pivot(index='state', columns='month_year', values='cfr')\n",
    "print(cfr_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "822a9c3c-a995-45d5-b2a3-09358355cfc9",
   "metadata": {},
   "source": [
    "Changing calculations to pandas (compute()) data makes logical sense due to the following considerations:\n",
    "\n",
    "The dataset has undergone significant reduction because of the applied state-level grouping process.\n",
    "The date operations and pivot operations require advanced and complex programming which Dask only partly supports\n",
    "At this stage the needed memory storage remains within a manageable range\n",
    "The expenses of executing distributed computations surpass what this approach delivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c63c9-3529-454a-90dd-a0c0bd45b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfr_changes = cfr_matrix.diff(axis=1)\n",
    "total_change = cfr_changes.abs().sum(axis=1)\n",
    "cfr_rankings = total_change.rank(ascending=True)\n",
    "print(cfr_rankings.sort_values())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5afed5e1-f990-4f9b-a327-06ecf3b4176b",
   "metadata": {},
   "source": [
    "The operations deal with matrices with dimensions of 50 states by 14 months while demanding complete row or column views to conduct computations. The added cost of distributing these operations exceeds their occurred benefits. The computational speed of pandas satisfies operational needs because it supports instant calculations and utilizes memory-based operations that perform reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43416882-0f99-4c0f-bff8-c43b5f09d0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80601b3-6396-4386-a684-41483ffedd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
